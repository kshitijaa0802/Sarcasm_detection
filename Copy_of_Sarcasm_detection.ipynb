{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLKFYnHFyTOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "d15ea757-3c2e-4db2-eaf3-e3178bd96338"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config\n",
        "!pip install jsonlines\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import h5py\n",
        "import jsonlines\n",
        "import nltk\n",
        "import config\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import Any, Iterable, Mapping, MutableSequence, Optional, Sequence, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DMs7e_-7yfNz",
        "outputId": "4c8001e0-fe14-4d28-f337-ee4faa8d50a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting config\n",
            "  Downloading config-0.5.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: config\n",
            "Successfully installed config-0.5.1\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # Paths for datasets and feature files\n",
        "        self.data_path = '/content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/sarcasm_data.json'\n",
        "        self.audio_features_path = '/content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/audio_features.p'\n",
        "        self.utterance_video_features_path = '/content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/features/utterances_final/resnet_pool5.hdf5'\n",
        "        self.context_video_features_path = '/content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/features/context_final/resnet_pool5 (2).hdf5'\n",
        "        self.text_embeddings_path = '/content/drive/MyDrive/project/MUStARD-master/MUStARD-master/BERT_text_features/bert-output.jsonl'\n",
        "        self.context_embeddings_path = '/content/drive/MyDrive/project/MUStARD-master/MUStARD-master/BERT_text_features/bert-output-context.jsonl'\n",
        "        self.batch_size = 32\n",
        "        self.num_epochs = 10\n",
        "\n",
        "        # Additional configuration\n",
        "        self.max_audio_frames = 300\n",
        "        self.fixed_audio_features = 12\n",
        "        self.max_video_frames = 96\n",
        "        self.video_feature_dim = 2048\n",
        "        self.text_embedding_dim = 768\n",
        "\n",
        "\n",
        "class MultimodalDataLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data_input = []\n",
        "        self.data_output = []\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Load and preprocess the dataset (text, audio, video).\n",
        "        \"\"\"\n",
        "        print(\"Loading data...\")\n",
        "        # Load the text data (utterances, context)\n",
        "        with open(self.config.data_path) as file:\n",
        "            dataset_dict = json.load(file)\n",
        "        print(\"Dataset loaded with\", len(dataset_dict), \"entries.\")\n",
        "\n",
        "        # Load BERT embeddings for text\n",
        "        text_bert_embeddings = self.load_bert_embeddings(self.config.text_embeddings_path)\n",
        "        context_bert_embeddings = self.load_bert_embeddings(self.config.context_embeddings_path)\n",
        "\n",
        "        # Load audio features (pickled data)\n",
        "        audio_features = self.load_audio_features(self.config.audio_features_path)\n",
        "\n",
        "        # Load video features for utterances and context (HDF5)\n",
        "        utterance_video_features = self.load_video_features(self.config.utterance_video_features_path)\n",
        "        context_video_features = self.load_video_features(self.config.context_video_features_path)\n",
        "\n",
        "        # Prepare data\n",
        "        self.parse_data(\n",
        "            dataset_dict,\n",
        "            audio_features,\n",
        "            utterance_video_features,\n",
        "            context_video_features,\n",
        "            text_bert_embeddings,\n",
        "            context_bert_embeddings,\n",
        "        )\n",
        "\n",
        "    def load_bert_embeddings(self, path):\n",
        "        \"\"\"\n",
        "        Load BERT embeddings for text or context.\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        print(\"Loading BERT embeddings from:\", path)\n",
        "        with jsonlines.open(path) as file:\n",
        "            for entry in file:\n",
        "                embeddings.append(np.array(entry['features'][0]['layers'][0]['values']))\n",
        "        print(\"Loaded\", len(embeddings), \"BERT embeddings.\")\n",
        "        return embeddings\n",
        "\n",
        "    def load_audio_features(self, path):\n",
        "        \"\"\"\n",
        "        Load audio features from a pickle file.\n",
        "        \"\"\"\n",
        "        print(\"Loading audio features from:\", path)\n",
        "        with open(path, 'rb') as file:\n",
        "            return pickle.load(file, encoding='latin1')\n",
        "\n",
        "    def load_video_features(self, path):\n",
        "        \"\"\"\n",
        "        Load video features from an HDF5 file.\n",
        "        \"\"\"\n",
        "        print(\"Loading video features from:\", path)\n",
        "        with h5py.File(path, 'r') as file:\n",
        "            return {key: file[key][:] for key in file}\n",
        "\n",
        "    def parse_data(self, dataset_dict, audio_features, utterance_video_features, context_video_features, text_embeddings, context_embeddings):\n",
        "        \"\"\"\n",
        "        Prepare the dataset into an appropriate format for attention-based models.\n",
        "        \"\"\"\n",
        "        print(\"Parsing data...\")\n",
        "        def fix_audio_features(audio, fixed_features=12, max_frames=300):\n",
        "            if audio is None:\n",
        "                return np.zeros((max_frames, fixed_features))\n",
        "            fixed_audio = np.zeros((max_frames, fixed_features))\n",
        "            num_frames = min(audio.shape[0], max_frames)\n",
        "            features_to_copy = min(audio.shape[1], fixed_features)\n",
        "            fixed_audio[:num_frames, :features_to_copy] = audio[:num_frames, :features_to_copy]\n",
        "            return fixed_audio\n",
        "\n",
        "        def fix_video_features(video, max_frames=96, feature_dim=2048):\n",
        "            if video is None:\n",
        "                return np.zeros((max_frames, feature_dim))\n",
        "            fixed_video = np.zeros((max_frames, feature_dim))\n",
        "            num_frames = min(video.shape[0], max_frames)\n",
        "            fixed_video[:num_frames] = video[:num_frames]\n",
        "            return fixed_video\n",
        "\n",
        "        def fix_text_embedding(embedding, embedding_dim=768):\n",
        "            if embedding is None or len(embedding) != embedding_dim:\n",
        "                return np.zeros(embedding_dim)\n",
        "            return embedding\n",
        "\n",
        "        for idx, id_ in enumerate(dataset_dict):\n",
        "            self.data_input.append({\n",
        "                \"utterance\": dataset_dict[id_].get(\"utterance\", \"\"),\n",
        "                \"context\": dataset_dict[id_].get(\"context\", []),\n",
        "                \"audio\": fix_audio_features(audio_features.get(id_)),\n",
        "                \"utterance_video\": fix_video_features(utterance_video_features.get(id_)),\n",
        "                \"context_video\": fix_video_features(context_video_features.get(id_)),\n",
        "                \"text_embedding\": fix_text_embedding(text_embeddings[idx]),\n",
        "                \"context_embedding\": fix_text_embedding(context_embeddings[idx]),\n",
        "            })\n",
        "            self.data_output.append(dataset_dict[id_].get(\"sarcasm\", 0))\n",
        "        print(\"Data parsing complete. Total samples:\", len(self.data_input))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kswNAeOGEmvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, data_input, data_output):\n",
        "        self.data_input = data_input\n",
        "        self.data_output = data_output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_output)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_input[idx], self.data_output[idx]\n",
        "\n",
        "\n",
        "def multimodal_collate_fn(batch):\n",
        "    features, labels = zip(*batch)\n",
        "\n",
        "    # Text features (shape: [batch_size, text_dim])\n",
        "    text = torch.tensor([f[\"text_embedding\"] for f in features], dtype=torch.float32)\n",
        "\n",
        "    # Context features (shape: [batch_size, context_dim])\n",
        "    context = torch.tensor([f[\"context_embedding\"] for f in features], dtype=torch.float32)\n",
        "\n",
        "    # Audio features (mean pooling along frames)\n",
        "    audio = pad_sequence([torch.tensor(f[\"audio\"], dtype=torch.float32) for f in features], batch_first=True)\n",
        "    audio = audio.mean(dim=1)  # Mean pooling reduces shape to (batch_size, features_per_frame)\n",
        "\n",
        "    # Utterance video features (mean pooling along frames)\n",
        "    utterance_video = pad_sequence([torch.tensor(f[\"utterance_video\"], dtype=torch.float32) for f in features], batch_first=True)\n",
        "\n",
        "    # Context video features (mean pooling along frames)\n",
        "    context_video = pad_sequence([torch.tensor(f[\"context_video\"], dtype=torch.float32) for f in features], batch_first=True)\n",
        "    context_video = context_video.mean(dim=1)  # Mean pooling reduces shape to (batch_size, features_per_frame)\n",
        "\n",
        "    # Labels (shape: [batch_size])\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    print(f\"text shape: {text.shape}\")\n",
        "    print(f\"context shape: {context.shape}\")\n",
        "    print(f\"audio shape: {audio.shape}\")\n",
        "    print(f\"utterance_video shape: {utterance_video.mean(dim=1).shape}\")\n",
        "    print(f\"context_video shape: {context_video.shape}\")\n",
        "    print(f\"labels shape: {labels.shape}\")\n",
        "\n",
        "    return {\n",
        "        \"text\": text,  # Shape: (batch_size, text_dim)\n",
        "        \"context\": context,  # Shape: (batch_size, context_dim)\n",
        "        \"audio\": audio,  # Shape: (batch_size, features_per_frame)\n",
        "        \"utterance_video\": utterance_video.mean(dim=1),  # Shape: (batch_size, features_per_frame)\n",
        "        \"context_video\": context_video,  # Shape: (batch_size, features_per_frame)\n",
        "    }, labels\n"
      ],
      "metadata": {
        "id": "_qd-0ATdApO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "data_loader = MultimodalDataLoader(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLk23_LAEtgc",
        "outputId": "c0836fe5-1135-4151-d821-5179c7b5fe58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Dataset loaded with 690 entries.\n",
            "Loading BERT embeddings from: /content/drive/MyDrive/project/MUStARD-master/MUStARD-master/BERT_text_features/bert-output.jsonl\n",
            "Loaded 690 BERT embeddings.\n",
            "Loading BERT embeddings from: /content/drive/MyDrive/project/MUStARD-master/MUStARD-master/BERT_text_features/bert-output-context.jsonl\n",
            "Loaded 2261 BERT embeddings.\n",
            "Loading audio features from: /content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/audio_features.p\n",
            "Loading video features from: /content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/features/utterances_final/resnet_pool5.hdf5\n",
            "Loading video features from: /content/drive/MyDrive/project/MUStARD-master/MUStARD-master/data/features/context_final/resnet_pool5 (2).hdf5\n",
            "Parsing data...\n",
            "Data parsing complete. Total samples: 690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ns49rp7sEfpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Attention Mechanism (to focus on important parts of input features)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn_weights = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):  # inputs: (batch_size, seq_len, hidden_dim)\n",
        "        scores = self.attn_weights(inputs)  # (batch_size, seq_len, 1)\n",
        "        attn_weights = F.softmax(scores, dim=1)  # Normalize attention scores\n",
        "        context = torch.sum(inputs * attn_weights, dim=1)  # Weighted sum\n",
        "        return context, attn_weights"
      ],
      "metadata": {
        "id": "lJu9_kAQ4oBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultimodalSarcasmModel(nn.Module):\n",
        "    def __init__(self, text_dim, context_dim, audio_dim, video_dim, hidden_dim, output_dim):\n",
        "        super(MultimodalSarcasmModel, self).__init__()\n",
        "        # Fully connected layers for each modality\n",
        "        self.text_fc = nn.Linear(text_dim, hidden_dim)\n",
        "        self.context_fc = nn.Linear(context_dim, hidden_dim)\n",
        "        self.audio_fc = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.video_fc = nn.Linear(video_dim, hidden_dim)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Combined layer\n",
        "        combined_dim = hidden_dim * 5\n",
        "        self.final_fc = nn.Linear(combined_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, context, audio, utterance_video, context_video):\n",
        "        # Debugging input shapes\n",
        "        print(f\"Original text shape: {text.shape}\")\n",
        "        print(f\"Original context shape: {context.shape}\")\n",
        "        print(f\"Original audio shape: {audio.shape}\")\n",
        "        print(f\"Original utterance_video shape: {utterance_video.shape}\")\n",
        "        print(f\"Original context_video shape: {context_video.shape}\")\n",
        "\n",
        "        # Process text and context (no reshape needed)\n",
        "        text_out = F.relu(self.text_fc(text))  # Shape: [batch_size, hidden_dim]\n",
        "        context_out = F.relu(self.context_fc(context))  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        # Reshape audio if needed for multiplication\n",
        "        if len(audio.shape) > 2:  # Example: audio shape is [batch_size, seq_len, feature_dim]\n",
        "            audio = torch.mean(audio, dim=1)  # Apply mean pooling along seq_len\n",
        "        print(f\"Reshaped audio shape: {audio.shape}\")\n",
        "        audio_out = F.relu(self.audio_fc(audio))  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        # Reshape video features if needed\n",
        "        if len(utterance_video.shape) > 2:  # Example: utterance_video shape is [batch_size, seq_len, feature_dim]\n",
        "            utterance_video = utterance_video.mean(dim=1)  # Apply mean pooling along seq_len\n",
        "        print(f\"Reshaped utterance_video shape: {utterance_video.shape}\")\n",
        "        utterance_video_out = F.relu(self.video_fc(utterance_video))  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        if len(context_video.shape) > 2:  # Example: context_video shape is [batch_size, seq_len, feature_dim]\n",
        "            context_video = context_video.mean(dim=1)  # Apply mean pooling along seq_len\n",
        "        print(f\"Reshaped context_video shape: {context_video.shape}\")\n",
        "        context_video_out = F.relu(self.video_fc(context_video))  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat([text_out, context_out, audio_out, utterance_video_out, context_video_out], dim=-1)\n",
        "        print(f\"Combined feature shape: {combined.shape}\")  # Should match [batch_size, hidden_dim * 5]\n",
        "\n",
        "        # Final output\n",
        "        output = self.final_fc(combined)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "q4BO-Xg4gDKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tensors\n",
        "batch_size = 32\n",
        "text_tensor = torch.randn(batch_size, 768)  # Shape: [batch_size, text_dim]\n",
        "context_tensor = torch.randn(batch_size, 768)  # Shape: [batch_size, context_dim]\n",
        "audio_tensor = torch.randn(batch_size, 300, 12)  # Shape: [batch_size, seq_len, audio_dim]\n",
        "utterance_video_tensor = torch.randn(batch_size, 96, 2048)  # Shape: [batch_size, seq_len, video_dim]\n",
        "context_video_tensor = torch.randn(batch_size, 96, 2048)  # Shape: [batch_size, seq_len, video_dim]\n"
      ],
      "metadata": {
        "id": "S2SLVGm0gHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tensor = text_tensor.to(device)\n",
        "context_tensor = context_tensor.to(device)\n",
        "audio_tensor = audio_tensor.to(device)\n",
        "utterance_video_tensor = utterance_video_tensor.to(device)\n",
        "context_video_tensor = context_video_tensor.to(device)"
      ],
      "metadata": {
        "id": "gHYPuXyIFE_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and test the model\n",
        "model = MultimodalSarcasmModel(\n",
        "    text_dim=768,\n",
        "    context_dim=768,\n",
        "    audio_dim=12,\n",
        "    video_dim=2048,\n",
        "    hidden_dim=512,\n",
        "    output_dim=2\n",
        ")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "VhQQ91higLSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7018b691-be4d-4426-8965-a94d9c89d149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultimodalSarcasmModel(\n",
              "  (text_fc): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (context_fc): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (audio_fc): Linear(in_features=12, out_features=512, bias=True)\n",
              "  (video_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (attention): Attention(\n",
              "    (attn_weights): Linear(in_features=512, out_features=1, bias=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (final_fc): Linear(in_features=2560, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmN_WHEGFGIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "outputs = model(text_tensor, context_tensor, audio_tensor, utterance_video_tensor, context_video_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ATA-1jwgOw8",
        "outputId": "2480b563-e9c0-4de5-9ddf-4a8cea5bfeaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 300, 12])\n",
            "Original utterance_video shape: torch.Size([32, 96, 2048])\n",
            "Original context_video shape: torch.Size([32, 96, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output debugging\n",
        "print(f\"Output shape: {outputs.shape}\")  # Should be [batch_size, output_dim]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUftgYVMgU8i",
        "outputId": "deffa035-cb5f-4c87-adf2-fb0a7eb54e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([32, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataset\n",
        "dataset = MultimodalDataset(data_loader.data_input, data_loader.data_output)"
      ],
      "metadata": {
        "id": "BMIxLtVQONH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Assuming `data_loader.data_input` and `data_loader.data_output` are already loaded\n",
        "train_size = int(0.8 * len(data_loader.data_input))  # 80% training data\n",
        "val_size = len(data_loader.data_input) - train_size  # 20% validation data\n",
        "indices = list(range(len(data_loader.data_input)))\n",
        "\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "\n",
        "# # Create datasets for train and validation\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=multimodal_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=multimodal_collate_fn)\n"
      ],
      "metadata": {
        "id": "5T5hFv9bhytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-ON37DEhDag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "features, labels = next(iter(train_loader))\n",
        "\n",
        "# Assuming labels are a PyTorch tensor\n",
        "labels_numpy = labels.cpu().numpy()  # Convert to NumPy array if it's a tensor\n",
        "\n",
        "\n",
        "# Define the weighted loss function and optimizer\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels_numpy)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Initialize variables to track the best accuracy and early stopping\n",
        "best_accuracy = 0.0  # To store the highest validation accuracy\n",
        "best_f1 = 0.0\n",
        "patience = 12  # Number of epochs to wait for improvement\n",
        "epochs_without_improvement = 0  # Counter for early stopping\n",
        "\n",
        "# Learning rate scheduler (if needed)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, verbose=True)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Increased number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Unpack features and labels from the batch\n",
        "        features, labels = batch\n",
        "\n",
        "        # Move features and labels to the device (e.g., GPU/CPU)\n",
        "        features = {key: value.to(device) for key, value in features.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = model(\n",
        "            features[\"text\"],\n",
        "            features[\"context\"],\n",
        "            features[\"audio\"],\n",
        "            features[\"utterance_video\"],\n",
        "            features[\"context_video\"]\n",
        "        )\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Log epoch loss\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Validation step: Evaluate the model on the validation set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:  # Validation loader\n",
        "            val_features, val_labels = val_batch\n",
        "            val_features = {key: value.to(device) for key, value in val_features.items()}\n",
        "            val_labels = val_labels.to(device)\n",
        "\n",
        "            val_outputs = model(\n",
        "                val_features[\"text\"],\n",
        "                val_features[\"context\"],\n",
        "                val_features[\"audio\"],\n",
        "                val_features[\"utterance_video\"],\n",
        "                val_features[\"context_video\"]\n",
        "            )\n",
        "\n",
        "            # Get predicted labels\n",
        "            _, predicted = torch.max(val_outputs, 1)\n",
        "            correct += (predicted == val_labels).sum().item()\n",
        "            total += val_labels.size(0)\n",
        "\n",
        "            # Collect predictions and true labels for F1 calculation\n",
        "            val_predictions.extend(predicted.cpu().numpy())\n",
        "            val_true_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy and F1-Score\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
        "\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "    print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_accuracy or val_f1 > best_f1:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_f1 = max(best_f1, val_f1)\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"New best model saved with accuracy: {best_accuracy:.2f}% and F1-Score: {best_f1:.4f}\")\n",
        "        epochs_without_improvement = 0  # Reset counter\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    # Early stopping: Stop training if no improvement after 'patience' epochs\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(\"Early stopping due to no improvement in validation accuracy.\")\n",
        "        break\n",
        "\n",
        "    # Update learning rate using scheduler\n",
        "    scheduler.step(val_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTFYOkneSIF4",
        "outputId": "fb4ddb3b-dc17-48b0-ad14-24912dc498f3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 5/20, Loss: 0.5011\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 59.42%\n",
            "Validation F1-Score: 0.5783\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 6/20, Loss: 0.4946\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 63.77%\n",
            "Validation F1-Score: 0.6336\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 7/20, Loss: 0.4365\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 73.19%\n",
            "Validation F1-Score: 0.7265\n",
            "New best model saved with accuracy: 73.19% and F1-Score: 0.7265\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 8/20, Loss: 0.3932\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 69.57%\n",
            "Validation F1-Score: 0.6993\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 9/20, Loss: 0.3381\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 70.29%\n",
            "Validation F1-Score: 0.7059\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 10/20, Loss: 0.2863\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 68.12%\n",
            "Validation F1-Score: 0.6829\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 11/20, Loss: 0.2525\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 64.49%\n",
            "Validation F1-Score: 0.6384\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 12/20, Loss: 0.2164\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 58.70%\n",
            "Validation F1-Score: 0.5632\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 13/20, Loss: 0.2291\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 52.90%\n",
            "Validation F1-Score: 0.4744\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 14/20, Loss: 0.2002\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 70.29%\n",
            "Validation F1-Score: 0.7042\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 15/20, Loss: 0.1549\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 68.84%\n",
            "Validation F1-Score: 0.6915\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 16/20, Loss: 0.1351\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 71.01%\n",
            "Validation F1-Score: 0.7136\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 17/20, Loss: 0.1335\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 71.01%\n",
            "Validation F1-Score: 0.7136\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 18/20, Loss: 0.1299\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 71.74%\n",
            "Validation F1-Score: 0.7207\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([8, 768])\n",
            "context shape: torch.Size([8, 768])\n",
            "audio shape: torch.Size([8, 12])\n",
            "utterance_video shape: torch.Size([8, 2048])\n",
            "context_video shape: torch.Size([8, 2048])\n",
            "labels shape: torch.Size([8])\n",
            "Original text shape: torch.Size([8, 768])\n",
            "Original context shape: torch.Size([8, 768])\n",
            "Original audio shape: torch.Size([8, 12])\n",
            "Original utterance_video shape: torch.Size([8, 2048])\n",
            "Original context_video shape: torch.Size([8, 2048])\n",
            "Reshaped audio shape: torch.Size([8, 12])\n",
            "Reshaped utterance_video shape: torch.Size([8, 2048])\n",
            "Reshaped context_video shape: torch.Size([8, 2048])\n",
            "Combined feature shape: torch.Size([8, 2560])\n",
            "Epoch 19/20, Loss: 0.1240\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([32, 768])\n",
            "context shape: torch.Size([32, 768])\n",
            "audio shape: torch.Size([32, 12])\n",
            "utterance_video shape: torch.Size([32, 2048])\n",
            "context_video shape: torch.Size([32, 2048])\n",
            "labels shape: torch.Size([32])\n",
            "Original text shape: torch.Size([32, 768])\n",
            "Original context shape: torch.Size([32, 768])\n",
            "Original audio shape: torch.Size([32, 12])\n",
            "Original utterance_video shape: torch.Size([32, 2048])\n",
            "Original context_video shape: torch.Size([32, 2048])\n",
            "Reshaped audio shape: torch.Size([32, 12])\n",
            "Reshaped utterance_video shape: torch.Size([32, 2048])\n",
            "Reshaped context_video shape: torch.Size([32, 2048])\n",
            "Combined feature shape: torch.Size([32, 2560])\n",
            "text shape: torch.Size([10, 768])\n",
            "context shape: torch.Size([10, 768])\n",
            "audio shape: torch.Size([10, 12])\n",
            "utterance_video shape: torch.Size([10, 2048])\n",
            "context_video shape: torch.Size([10, 2048])\n",
            "labels shape: torch.Size([10])\n",
            "Original text shape: torch.Size([10, 768])\n",
            "Original context shape: torch.Size([10, 768])\n",
            "Original audio shape: torch.Size([10, 12])\n",
            "Original utterance_video shape: torch.Size([10, 2048])\n",
            "Original context_video shape: torch.Size([10, 2048])\n",
            "Reshaped audio shape: torch.Size([10, 12])\n",
            "Reshaped utterance_video shape: torch.Size([10, 2048])\n",
            "Reshaped context_video shape: torch.Size([10, 2048])\n",
            "Combined feature shape: torch.Size([10, 2560])\n",
            "Validation Accuracy: 71.01%\n",
            "Validation F1-Score: 0.7135\n",
            "Early stopping due to no improvement in validation accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(val_true_labels, val_predictions)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report\n",
        "report = classification_report(val_true_labels, val_predictions, target_names=[\"Class 0\", \"Class 1\"])\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "hJvCSvoKVD6e",
        "outputId": "419db36c-d178-44e5-ca12-dcda17c57623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[43 11]\n",
            " [29 55]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.60      0.80      0.68        54\n",
            "     Class 1       0.83      0.65      0.73        84\n",
            "\n",
            "    accuracy                           0.71       138\n",
            "   macro avg       0.72      0.73      0.71       138\n",
            "weighted avg       0.74      0.71      0.71       138\n",
            "\n"
          ]
        }
      ]
    }
  ]
}